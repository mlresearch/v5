---
title: Learning Exercise Policies for American Options
abstract: Options are important instruments in modern finance. In this paper, we investigate
  reinforcement learning (RL) methods—in particular, least-squares policy iteration
  (LSPI)—for the problem of learning exercise policies for American options. We develop
  finite-time bounds on the performance of the policy obtained with LSPI and compare
  LSPI and the fitted Q-iteration algorithm (FQI) with the Longstaff-Schwartz method
  (LSM), the standard least-squares Monte Carlo algorithm from the finance community.
  Our empirical results show that the exercise policies discovered by LSPI and FQI
  gain larger payoffs than those discovered by LSM, on both real and synthetic data.
  Furthermore, we find that for all methods the policies learned from real data generally
  gain similar payoffs to the policies learned from simulated data. Our work shows
  that solution methods developed in machine learning can advance the state-of-the-art
  in an important and challenging application area, while demonstrating that computational
  finance remains a promising area for future applications of machine learning methods.
pdf: http://proceedings.pmlr.press/li09d/li09d.pdf
layout: inproceedings
id: li09d
month: 0
firstpage: 352
lastpage: 359
page: 352-359
origpdf: http://jmlr.org/proceedings/papers/v5/li09d/li09d.pdf
sections: 
author:
- given: Yuxi
  family: Li
- given: Csaba
  family: Szepesvari
- given: Dale
  family: Schuurmans
date: 2009-04-15
publisher: PMLR
container-title: Proceedings of the Twelth International Conference on Artificial
  Intelligence and Statistics
volume: '5'
genre: inproceedings
issued:
  date-parts:
  - 2009
  - 4
  - 15
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
