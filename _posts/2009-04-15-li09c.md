---
title: Tighter and Convex Maximum Margin Clustering
abstract: Maximum margin principle  has been successfully applied to many  supervised
  and semi-supervised problems in machine learning.  Recently, this principle was
  extended for clustering, referred to as  Maximum Margin Clustering (MMC) and achieved
  promising performance  in recent studies. To avoid the problem of local minima,
  MMC can be  solved globally via convex semi-definite programming (SDP)  relaxation.
  Although many efficient approaches have been proposed to  alleviate the computational
  burden of SDP, convex MMCs are still not  scalable for medium data sets. In this
  paper, we propose a novel  convex optimization method, LG-MMC, which maximizes the
  margin of  opposite clusters via label generation. It can be shown that LG-MMC  is
  much more scalable than existing convex approaches. Moreover, we  show that our
  convex relaxation is tighter than state-of-art convex  MMCs. Experiments on eighteen
  UCI datasets and MNIST dataset show  significant improvement over existing MMC algorithms.
pdf: "./li09c/li09c.pdf"
layout: inproceedings
key: li09c
month: 0
firstpage: 344
lastpage: 351
origpdf: http://jmlr.org/proceedings/papers/v5/li09c/li09c.pdf
sections: 
authors:
- given: Yu-Feng
  family: Li
- given: Ivor W.
  family: Tsang
- given: Jame
  family: Kwok
- given: Zhi-Hua
  family: Zhou
---
