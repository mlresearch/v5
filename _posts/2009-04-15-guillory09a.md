---
title: Active Learning as Non-Convex Optimization
abstract: 'We propose a new view of active learning algorithms as optimization. We
  show that many online active learning algorithms can be viewed as stochastic gradient
  descent on non-convex objective functions. Variations of some of these algorithms
  and objective functions have been previously proposed without noting this connection.  We
  also point out a connection between the standard min-margin offline active learning
  algorithm and non-convex losses.  Finally, we discuss and show empirically how viewing
  active learning as non-convex loss minimization helps explain two previously observed
  phenomena: certain active learning algorithms achieve better generalization error
  than passive learning algorithms on certain data sets and on other data sets many
  active learning algorithms are prone to local minima.'
pdf: http://proceedings.mlr.press/v5/guillory09a/guillory09a.pdf
layout: inproceedings
id: guillory09a
month: 0
firstpage: 201
lastpage: 208
page: 201-208
sections: 
author:
- given: Andrew
  family: Guillory
- given: Erick
  family: Chastain
- given: Jeff
  family: Bilmes
date: 2009-04-15
address: Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA
publisher: PMLR
container-title: Proceedings of the Twelth International Conference on Artificial
  Intelligence and Statistics
volume: '5'
genre: inproceedings
issued:
  date-parts:
  - 2009
  - 4
  - 15
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
