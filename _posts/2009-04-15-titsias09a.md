---
title: Variational Learning of Inducing Variables in Sparse Gaussian Processes
abstract: Sparse Gaussian process methods that use inducing variables require the
  selection of the inducing inputs and the kernel hyperparameters. We introduce a
  variational formulation for sparse approximations that jointly infers the inducing
  inputs and the kernel hyperparameters by maximizing a lower bound of the true log
  marginal likelihood. The key property of this formulation is that the inducing inputs  are
  defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler
  divergence between  the variational distribution and the exact posterior distribution
  over the latent function values. We apply this technique to regression and we compare
  it with other approaches in the literature.
pdf: http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: titsias09a
month: 0
firstpage: '567'
lastpage: '574'
page: 567-574
sections: 
author:
- given: Michalis
  family: Titsias
date: 2009-04-15
address: Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA
publisher: PMLR
container-title: Proceedings of the Twelth International Conference on Artificial
  Intelligence and Statistics
volume: '5'
genre: inproceedings
issued:
  date-parts:
  - 2009
  - 4
  - 15
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
