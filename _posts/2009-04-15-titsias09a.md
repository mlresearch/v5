---
title: Variational Learning of Inducing Variables in Sparse Gaussian Processes
abstract: Sparse Gaussian process methods that use inducing variables require the
  selection of the inducing inputs and the kernel hyperparameters. We introduce a
  variational formulation for sparse approximations that jointly infers the inducing
  inputs and the kernel hyperparameters by maximizing a lower bound of the true log
  marginal likelihood. The key property of this formulation is that the inducing inputs  are
  defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler
  divergence between  the variational distribution and the exact posterior distribution
  over the latent function values. We apply this technique to regression and we compare
  it with other approaches in the literature.
pdf: "./titsias09a/titsias09a.pdf"
layout: inproceedings
key: titsias09a
month: 0
firstpage: 567
lastpage: 574
origpdf: http://jmlr.org/proceedings/papers/v5/titsias09a/titsias09a.pdf
sections: 
authors:
- given: Michalis
  family: Titsias
---
