---
title: Speed and Sparsity of Regularized Boosting
abstract: Boosting algorithms with l1 regularization are of interest because l1 regularization
  leads to sparser composite classifiers. Moreover, Rosset et al. have shown that
  for separable data, standard lp regularized loss minimization results in a margin
  maximizing classifier in the limit as regularization is relaxed. For the case p=1,
  we extend these results by obtaining explicit convergence bounds on the regularization
  required to yield a margin within prescribed accuracy of the maximum achievable
  margin. We derive similar rates of convergence for the epsilon AdaBoost algorithm,
  in the process providing a new proof that epsilon AdaBoost is margin maximizing
  as epsilon converges to 0. Because both of these known algorithms are computationally
  expensive, we introduce a new hybrid algorithm, AdaBoost+L1, that combines the virtues
  of AdaBoost with the sparsity of l1 regularization in a computationally efficient
  fashion. We prove that the algorithm is margin maximizing and empirically examine
  its performance on five datasets.
pdf: http://proceedings.pmlr.press/xi09a/xi09a.pdf
layout: inproceedings
id: xi09a
month: 0
firstpage: 615
lastpage: 622
page: 615-622
origpdf: http://jmlr.org/proceedings/papers/v5/xi09a/xi09a.pdf
sections: 
author:
- given: Yongxin
  family: Xi
- given: Zhen
  family: Xiang
- given: Peter
  family: Ramadge
- given: Robert
  family: Schapire
date: 2009-04-15
address: Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA
publisher: PMLR
container-title: Proceedings of the Twelth International Conference on Artificial
  Intelligence and Statistics
volume: '5'
genre: inproceedings
issued:
  date-parts:
  - 2009
  - 4
  - 15
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
