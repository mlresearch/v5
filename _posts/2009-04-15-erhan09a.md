---
title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
  Pre-Training
abstract: Whereas theoretical work suggests that deep architectures might be more
  efficient at representing highly-varying functions, training deep architectures  was
  unsuccessful until the recent advent of algorithms based on unsupervised pretraining.
  Even though these new algorithms have enabled training deep models, many questions
  remain as to the nature of this difficult learning problem. Answering these questions
  is important if learning in deep architectures is to be further improved. We attempt
  to shed some light on these questions through extensive simulations. The experiments
  confirm and clarify the advantage of unsupervised pre-training. They demonstrate
  the robustness of the training procedure with respect to the random initialization,
  the positive  effect of pre-training in terms of optimization and its role as a
  kind of regularizer. We show the influence of architecture depth, model capacity,
  and number of training examples.
pdf: http://proceedings.mlr.press/v5/erhan09a/erhan09a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: erhan09a
month: 0
tex_title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
  Pre-Training
firstpage: 153
lastpage: 160
page: 153-160
sections: 
author:
- given: Dumitru
  family: Erhan
- given: Pierre-Antoine
  family: Manzagol
- given: Yoshua
  family: Bengio
- given: Samy
  family: Bengio
- given: Pascal
  family: Vincent
date: 2009-04-15
address: Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA
publisher: PMLR
container-title: Proceedings of the Twelth International Conference on Artificial
  Intelligence and Statistics
volume: '5'
genre: inproceedings
issued:
  date-parts:
  - 2009
  - 4
  - 15
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
