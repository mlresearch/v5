---
title: Residual Splash for Optimally Parallelizing Belief Propagation
abstract: As computer architectures move towards parallelism we must build a    new
  theoretical understanding of parallelism in machine learning.    In this paper we
  focus on parallelizing message passing inference    algorithms in graphical models.
  We develop a theoretical    understanding of the limitations of parallelism in belief    propagation
  and bound the optimal achievable running parallel    performance on a certain class
  of graphical models.  We demonstrate    that the fully synchronous parallelization
  of belief propagation is    highly inefficient.  We provide a new parallel belief
  propagation    which achieves optimal performance on a certain class of graphical    models.  Using
  two challenging real-world problems, we empirically    evaluate the performance
  of our algorithm. On the real-world    problems, we find that our new algorithm
  achieves near linear    performance improvements and out performs alternative parallel    belief
  propagation algorithms.
pdf: http://proceedings.pmlr.press/gonzalez09a/gonzalez09a.pdf
layout: inproceedings
id: gonzalez09a
month: 0
firstpage: 177
lastpage: 184
page: 177-184
origpdf: http://jmlr.org/proceedings/papers/v5/gonzalez09a/gonzalez09a.pdf
sections: 
author:
- given: Joseph
  family: Gonzalez
- given: Yucheng
  family: Low
- given: Carlos
  family: Guestrin
date: 2009-04-15
publisher: PMLR
container-title: Proceedings of the Twelth International Conference on Artificial
  Intelligence and Statistics
volume: '5'
genre: inproceedings
issued:
  date-parts:
  - 2009
  - 4
  - 15
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
