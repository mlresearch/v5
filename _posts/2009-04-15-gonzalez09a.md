---
title: Residual Splash for Optimally Parallelizing Belief Propagation
abstract: As computer architectures move towards parallelism we must build a    new
  theoretical understanding of parallelism in machine learning.    In this paper we
  focus on parallelizing message passing inference    algorithms in graphical models.
  We develop a theoretical    understanding of the limitations of parallelism in belief    propagation
  and bound the optimal achievable running parallel    performance on a certain class
  of graphical models.  We demonstrate    that the fully synchronous parallelization
  of belief propagation is    highly inefficient.  We provide a new parallel belief
  propagation    which achieves optimal performance on a certain class of graphical    models.  Using
  two challenging real-world problems, we empirically    evaluate the performance
  of our algorithm. On the real-world    problems, we find that our new algorithm
  achieves near linear    performance improvements and out performs alternative parallel    belief
  propagation algorithms.
pdf: "./gonzalez09a/gonzalez09a.pdf"
layout: inproceedings
id: gonzalez09a
month: 0
firstpage: 177
lastpage: 184
page: 177-184
origpdf: http://jmlr.org/proceedings/papers/v5/gonzalez09a/gonzalez09a.pdf
sections: 
author:
- given: Joseph
  family: Gonzalez
- given: Yucheng
  family: Low
- given: Carlos
  family: Guestrin
date: '2009-04-15 00:02:57'
publisher: PMLR
---
